
# Import necessary libraries
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import seaborn as sns
import os, glob
import swifter
import scipy.signal as ss
from sklearn import preprocessing as pp
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from statsmodels.nonparametric.smoothers_lowess import lowess
matplotlib.rcParams['figure.figsize'] = [24, 12]

# Define a function for lowess smoothing
def make_lowess(series, frac=0, it=0, delta=0.0):
    endog = series.values
    exog = series.index
    print('Smoothing..')
    smooth = lowess(endog, exog, frac=frac, it=it, delta=delta)
    print('Done Smoothing..')
    index, data = np.transpose(smooth)
    return pd.Series(data, index=(pd.to_datetime(index).tz_localize('UTC').tz_convert('US/Central')))

# Define the date range for reading data
read_start = pd.to_datetime('2020-01-01T00:00:00')
read_end = pd.to_datetime('2020-04-30T00:00:00')

# Read monthly saved Data in Parquet File Format
daterange_monthlystart_list = pd.date_range(read_start, read_end, freq='MS').strftime("%Y-%m-%d %H:%M:%S.%f").tolist()
daterange_monthlyend_list = (pd.date_range(read_start, read_end, freq='M') + pd.offsets.DateOffset(days=1)).strftime("%Y-%m-%d %H:%M:%S.%f").tolist()
daterange_monthly_list = list(zip(daterange_monthlystart_list, daterange_monthlyend_list))

for i in range(len(daterange_monthly_list)):
    if (i == 0):
        print("Read monthly starts: ", "\t\t", "monthly ends: ")
    print(daterange_monthlystart_list[i], "\t", daterange_monthlyend_list[i])

# Read and concatenate tables
tables = []
for ps in (daterange_monthly_list):
    print('\nReading ' + ps[0][:7] + "_Sanding.parquet")
    %time
    table = pq.read_table("./Data/" + ps[0][:7] + "_Sanding.parquet",
                          columns=None,
                          use_pandas_metadata=True)
    tables.append(table)
    print('Done Reading...')
print('\nConcatenating Tables...')
%time my_data = pa.concat_tables(tables).to_pandas()

# Remove rows with duplicate indices after concatenating
my_data = my_data.loc[~my_data.index.duplicated(keep='first')]

# Define file location and format
file_location = "./Data/"
file_format = ".parquet"

# Define excluded columns from preprocessing
exclude_from_preprocessing = ['PRODUCTION', 'CHOKE-STEP-POS']

# Initialize final_data DataFrame
final_data = pd.DataFrame()

# Process data for each monthly period
for ps in (daterange_monthly_list):
    file = file_location + ps[0][:7] + "_Sanding" + file_format
    print('\nChecking file in the location ', file)
    train_data = pd.DataFrame()

    for col in my_data.columns:
        if col in (my_data.columns[~my_data.columns.isin(exclude_from_preprocessing)]):
            print('\n', col, "is in the pre-processing list")
            print('Preprocessing raw tag: ', col)
            period_daterange = pd.date_range(ps[0], ps[1], periods=5)
            monthly_data = []
            
            for x in range(len(period_daterange) - 1):
                print('Datetime from ', period_daterange[x], 'to', period_daterange[x + 1])
                df = my_data.loc[period_daterange[x]:period_daterange[x + 1]]
                if any(df.index.duplicated()):
                    print('\nRemoving rows with duplicate indices..\n')
                    df = df.loc[~df.index.duplicated(keep='first')]
                print('Downsampling to 5min')
                df = df.resample('5min').asfreq()
                print('Apply Smoothing')
                df = make_lowess(df[col], frac=0.3)
                print('Upsampling back to 5s')
                df = df.resample('5s').asfreq().interpolate(method='polynomial', order=3)
                print('Taking the first derivative')
                df = df.diff().fillna(method='ffill').fillna(method='bfill').div(5)
                print('Appending monthly data')
                monthly_data.append(df)
                
            print("Concat monthly data")
            big_data = pd.concat(monthly_data, axis=0)
            big_data.name = col
        else:
            print('\n', col, "is NOT in the pre-processing list")
            print('Reading directly...')
            big_data = my_data[col].loc[ps[0]:ps[1]]
        print('Concat features')
        train_data = pd.concat([train_data, big_data], axis=1)
    print('Done')
    final_data = pd.concat([final_data, train_data], axis=0)

# Define event start and end conditions
def func_event_start(row):
    return True if ((row.prev_index == 1.) and ((row['PRODUCTION'] == 0.) or (row['PRODUCTION'] > 100.))) else False

def func_event_end(row):
    return True if (((row.prev_index == 0.) or (row.prev_index > 100.)) and (row['PRODUCTION'] == 1.)) else False

# Identify event starts and ends
condition = final_data['PRODUCTION'].ne(final_data['PRODUCTION'].shift().bfill())
final_data["prev_index"] = final_data['PRODUCTION'].shift().bfill()

print("Identifying Event Starts..")
%time event_start = final_data[final_data.swifter.apply(func_event_start, axis=1)].index
print(event_start)
print("Identifying Event Ends..")
%time event_end = final_data[final_data.swifter.apply(func_event_end, axis=1)].index
print(event_end)
print("\nLength of Event Starts ", len(event_start))
print("\nLength of Event Ends ", len(event_end))

# Remove the "prev_index" column
df = final_data.drop(columns='prev_index')

# Define the event data for further processing
my_data_train = pd.DataFrame()
my_data_train_bad = pd.DataFrame()
for event_ind in range(len(event_start)):
    my_data_train_bad = my_data_train_bad.append(
        df[(event_start[event_ind] - pd.Timedelta(hours=12)): (event_end[event_ind] + pd.Timedelta(hours=24))])
my_data_train = df.loc[~df.index.isin(my_data_train_bad.index)]

# Perform PCA
X = my_data_train
pca = PCA(n_components=2
